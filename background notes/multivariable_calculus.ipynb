{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Background Note \n",
    "\n",
    "# Multivariable Calculus\n",
    "\n",
    "## By Albert S. \"Pete\" Kyle\n",
    "\n",
    "$\\def \\sm {} \\renewcommand{\\sm}{ {\\scriptstyle{\\text{*}}}}$ \n",
    "$\\def \\mm {} \\renewcommand{\\mm}{{\\scriptsize @}}$\n",
    "$\\def \\E {} \\renewcommand{\\E}{\\mathrm{E}}$\n",
    "$\\def \\e {} \\renewcommand{\\e}{\\mathrm{e}}$\n",
    "$\\def \\drm {} \\renewcommand{\\drm}{\\mathrm{\\, d}}$\n",
    "$\\def \\var {} \\renewcommand{\\var}{\\mathrm{var}}$\n",
    "$\\def \\cov {} \\renewcommand{\\cov}{\\mathrm{cov}}$\n",
    "$\\def \\corr {} \\renewcommand{\\corr}{\\mathrm{corr}}$\n",
    "$\\def \\stdev {} \\renewcommand{\\stdev}{\\mathrm{stdev}}$\n",
    "$\\def \\t {} \\renewcommand{\\t}{^{\\mathsf{T}}}$\n",
    "$\\def \\comma {} \\renewcommand{\\comma}{\\, , \\,}$\n",
    "$\\def \\vec {} \\renewcommand{\\vec}[1]{\\mathbf{#1}}$\n",
    "$\\def \\skew {} \\renewcommand{\\skew}{\\mathrm{skew}}$\n",
    "$\\def \\kurt {} \\renewcommand{\\kurt}{\\mathrm{kurt}}$\n",
    "$\\def \\prob {} \\renewcommand{\\prob}{\\textrm{prob}}$\n",
    "$\\def \\midx {} \\renewcommand{\\midx}{\\, \\mid \\,}$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version 3.11.8 | packaged by conda-forge | (main, Feb 16 2024, 20:40:50) [MSC v.1937 64 bit (AMD64)]\n",
      "NumPy version 1.26.4\n",
      "Timestamp: 2024-1023-1154\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import sys\n",
    "import datetime\n",
    "import timeit\n",
    "\n",
    "print('Python version ' + sys.version)\n",
    "print('NumPy version ' + np.__version__)\n",
    "\n",
    "timestamp = datetime.datetime.now().strftime('%Y-%m%d-%H%M')\n",
    "print(\"Timestamp:\", timestamp)\n",
    "tstart = timeit.default_timer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importance of calculus in finance\n",
    "\n",
    "Multivariable calculus has many different uses in finance:\n",
    "\n",
    "1. Multivariable calculus underlies many optimization techniques, including portfolio optimization, econometrics, machine learning, and neural networks.\n",
    "\n",
    "2. Calculus is important in probabiliy and statistics, which themselves are important for understanding the relationship between risk and return.\n",
    "\n",
    "3. Stochastic calculus and partial differential equations are important for derivatives pricing and risk management.\n",
    "\n",
    "4. Calculus is important for understanding hedging ratios, tradeoffs between marginal costs and benefits, equation solving related to deriving yield-to-maturity from bond prices, and other calculations related to the time value of money, such as **duration** and **convexity**..\n",
    "\n",
    "Calculus is useful because it simplifies both intution and calculations. The alternative to calculus is discrete approximations which can be opaque, complicated, and computationally inefficient.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Derivatives\n",
    "\n",
    "Let $f(x)$ denote some **univariate** function mapping points $x$ in a domain which is a subset of real numbers into a range which is also real numbers.  Let $a$ be some point in an open subset of the  domain of $f$. Then the **derivative** of $f$ at $a$, denoted  $f'(a)$, is  defined by\n",
    "\n",
    "$$\n",
    "f'(a) := \\lim_{h \\rightarrow 0} \\frac{f(a + h) - f(a) }{h},\n",
    "$$\n",
    "\n",
    "if this limit exists.  If the derivative exists at every point in the domain of the function $f$, then $f(x)$ is a **differentiable function** in its domain. For example, the functions $\\log(x)$ and $\\sqrt{x}$ are both differentiable in the open domain $(0, +\\infty)$, but neither is differentiable in the domain $[0, +\\infty)$ because $\\log(x)$ is undefined at $x=0,$  and the derivative of $\\sqrt{x}$ becomes infinite as $x \\rightarrow 0$. The **absolute value** function $f(x) := | x |$ is differentiable for all real numbers $x \\in (-\\infty, +\\infty)$ except $x=0$, where the graph of the function has a kink.\n",
    "\n",
    "The derivative is the slope to the tangent line to the graph of the function at the point $a$. Considering the point $a$ to be a constant, the tangent line is the graph of the function of $x$ defined by $f(a) + f'(a) \\sm (x - a)$. \n",
    "\n",
    "An alternative definition of the derivative is that $f'(a)$ is the derivative of $f$ at $a$ if\n",
    "\n",
    "$$\n",
    "f(x) = f(a) + f'(a) \\sm (x - a) + \\epsilon(x) \\sm (x- a ), \\qquad \\text{where } \\epsilon(x) \\rightarrow 0  \\text{ as } x \\rightarrow a.\n",
    "$$\n",
    "\n",
    "The function $\\epsilon(x)$ is the error of the tangent line, $f(a) + f'(a) \\sm (x - a)$, as an approximation to the function $f(x)$. This definition says that the tangent line is an arbitrarily accurate linear approximation to $f(x)$ in a sufficiently small neighborhood around $a$. Intuitively, if examined under a microscope at a very small scale, the graph of the potentially nonlinear  function $f(x)$ and the graph of the linear function defined by the derivative-implied tangent line would look almost identical.\n",
    "\n",
    "If a function is differentiable at a point, it is also continuous at that point. A function which is continuous in an interval need not be differentiable anywhere in that interval. A finance example of a nowhere-differentiable-but-continuous function is the path of a **Brownian motion** process.  A function is **continuously differentiable** if it is a differentiable function and the derivative itself is a continuous function.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradients\n",
    "\n",
    "Now suppose $f(\\vec{x})$ is a function which maps vectors $\\vec{x}$ in a subset of $\\mathbb{R^N}$ into **scalars** which are real numbers. The function $f(\\vec{x})$ is **differentiable** at the vector point $\\vec{a}$ if there exists a **gradient vector**, denoted $\\nabla f(\\vec{a})$ such that\n",
    "\n",
    "$$\n",
    "f(\\vec{x}) = f(\\vec{a}) + \\nabla f(\\vec{a}) \\mm (\\vec{x} - \\vec{a}) + \\epsilon(\\vec{x}) \\sm |\\vec{x} - \\vec{a}|,\n",
    "\\qquad \\text{where } \\epsilon(\\vec{x}) \\rightarrow 0 \\text{ as } \\vec{x} \\rightarrow \\vec{a}.\n",
    "$$\n",
    "\n",
    "In this definition, $\\nabla f(\\vec{a})$ is a vector of dimension $N$ which defines a **linear functional** mapping $N$-vectors $\\vec{v}$ to the scalar values $\\nabla f(\\vec{a}) \\mm \\vec{v}$.  The function of $\\vec{x}$ defined by $f(\\vec{a}) + \\nabla f(\\vec{a}) \\mm (\\vec{x} - \\vec{a})$ is a linear approximation to the function $f(\\vec{x})$. The scalar-valued function $\\epsilon(\\vec{x})$ defines the error of this approximation as a fraction of of the **norm** (length) of $\\vec{x} - \\vec{a}$. This norm is the **Euclidean norm** defined by $|\\vec{z}| = \\sqrt{\\vec{z}[0]^2 + \\vec{z}[1]^2 + \\ldots + \\vec{z}[N-1]^2}$, with $\\vec{z} := (\\vec{z}[0], \\vec{z}[1], \\ldots, \\vec{z}[N-1])$. The definition of the gradient says that the linear approximation becomes arbitrarily accurate in a small enough neighborhood of $\\vec{a}$.\n",
    "\n",
    "The gradient symbol $\\nabla$, which looks like an upside-down capital-delta ($\\Delta$) symbol,  is often called **nabla**.\n",
    "\n",
    "In one dimension, the linear approximation defined by the derivative of a univariate function is a tangent line.  In two dimensions, the two-dimensional gradient vector defines a tangent plane to a function whose domain is a subset of a plane.  In three or more dimensions, the linear approximation defines a **hyperplane**, which is hard to grasp intuitively but satisfies many of the intuitive properties of the one- or two-dimensional cases.\n",
    "\n",
    "The $N$ components of the gradient vector are **partial derivatives** of the the function $f(\\vec{x})$. The $n\\!$ th partial derivative of $f(\\vec{x})$ is the derivative of the univariate function defined by freezing all of the values of $\\vec{x}$ except for the $n\\!$ th one. For example, in three dimensions, we can define the univariate function $f_0(x) := f((x, \\vec{a}[1], \\vec{a}[2]))$. The $0\\!$ th (or first) partial derivative is $f_0'(x)$, which is often denoted $\\partial f(\\vec{x}) / \\partial x_0$. In the case of three dimensions, there are three partial derivatives, defined analogously as $\\partial f(\\vec{x}) / \\partial x_0$, $\\partial f(\\vec{x}) / \\partial x_1$, $\\partial f(\\vec{x}) / \\partial x_2$. (Note that here I have followed the Python convention of indexing from $0$ to $N-1$; the more standard convention is to index from $1$ to $N$, but the idea is the same.)\n",
    "\n",
    "To summarize, generalizing the concept of a univariate derivative to an arbitrarily large finite number of dimensions is straightforward. It simply requires generalizing the concept of a tangent line defined by a linear function of one variable to a tangent hyperplane defined by a linear function of many variables.  This linear function is itself defined by a gradient vector which generalizes the concept of a scalar derivative to multile dimensions.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rotation and rescaling\n",
    "\n",
    "One conceptual issue concerns the distinction between a gradient vector considered as a linear functional and the gradient vector interpreted as a vector of specific numbers each of which is a partial derivative. Consider a function defined on a two-dimensional space to which we give a geographic interpretation.  We can impose on the the geography a Cartesian coordinate system in which the positive vertical axis points north, the positive horizontal axis points east, and distances are measured in miles. There is, however, no theoretical reason why we could not have the vertical axis point in a different direction and measure distances in kilometers instead of miles. Changing the coordinate system would not change a function which mapped each geographic point to its altitude above sea level, nor would it change the gradient vectors, which have interpretations as directions of steepest ascent (uphill). The change in coordinates would change the numerical values of the partial derivatives, which are numbers defined with respect to a specific coordinate system which defines what points north and defines whether distance is measured in miles, kilometers, or something else.\n",
    "\n",
    "This conceptual issue is also important in finance. Portfolios of assets are often defined as linear combinations of assets, just as points on a map are defined by distances in north-south and east-west directions. Portfolios of assets can be rotated and rescaled just like geographic measurements. This is done for various reasons, including intuitive interpretation and numerical stability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jacobian\n",
    "\n",
    "Now let $\\vec{f} : \\mathbb{R}^N \\rightarrow \\mathbb{R}^M$ denote a function mapping $N$-vectors into $M$-vectors. We can think of $\\vec{f}$ as $M$ separate functions $f_1$, $\\ldots$, $F_M$ stacked together. If the function $\\vec{f}$ is differentiable at a vector-valued point $\\vec{x}$, the **Jacobian matrix** is defined as the matrix of partial derivatives:\n",
    "\n",
    "$$\n",
    "\\vec{J}(\\vec{x}) = \n",
    "\\left[ \\begin{array}{c c c }\n",
    "\\dfrac{\\partial{f_1(\\vec{x})}}{\\partial x_1} & \\ldots & \\dfrac{\\partial{f_1(\\vec{x})}}{\\partial x_N} \\\\  \n",
    "\\vdots & \\ddots & \\vdots \\\\\n",
    "\\dfrac{\\partial{f_M(\\vec{x})}}{\\partial x_M} & \\ldots & \\dfrac{\\partial{f_M(\\vec{x})}}{\\partial x_N} \\\\  \n",
    "\\end{array} \\right] .\n",
    "$$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bookkeeping issue: rows or columns?\n",
    "\n",
    "One bookkeeping issue concerns whether to think of gradients as row vectors or columns vectors. A related issue is whether the Jacobian matrix of the function $\\vec{f} : \\mathbb{R}^N \\rightarrow \\mathbb{R}^M$ is and $N \\times M$ matrix or the transposed $M \\times N$ matrix.  In the definition of the Jacobian above, the rows of the matrix correspond to the gradients of the component functions $f_1$, $\\ldots$, $f_M$. This suggests a convention which defines the gradient vector as a row vector.  \n",
    "\n",
    "In fact, the gradient vector is sometimes defined as a row vector and sometimes defined as a column vector. Similarly, the Jacobian matrix is sometimes defined as an $N \\times M$ matrix and sometimes as an $M \\times N$ matrix. Of course, the specific convention used does not ultimately matter. Nevertheless, this bookkeeping issue can be a source of confusion.\n",
    "\n",
    "For exmaple, it arises when defining notation for derivatives of linear and quadratic functions (as discussed below)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Second derivative, Hessian\n",
    "\n",
    "The **second derivative** of a univariate function is simply the derivative of the first derivative.\n",
    "\n",
    "Analogously, the second derivative of a function $f:\\mathbb{R}^N \\rightarrow \\mathbb{R}^1$ is the Jacobian of the gradient. Since the gradient is a mapping from $\\mathbb{R}^N$ into a gradient vector also in $\\mathbb{R}^N$, the second derivative can be represented as a square matrix of size $N \\times N$, the elements of which are the **second partial derivatives** (partial derivatives of partial derivatives) of $f$ evaluated at some point $\\vec{x}$. This matrix is called the **Hessian** matrix:\n",
    "\n",
    "$$\n",
    "\\vec{H}(\\vec{x}) = \n",
    "\\left[ \\begin{array}{c c c }\n",
    "\\dfrac{\\partial^2{f(\\vec{x})}}{\\partial x_0^2} & \\ldots & \\dfrac{\\partial^2{f(\\vec{x})}}{\\partial x_0 \\partial x_{N-1}} \\\\  \n",
    "\\vdots & \\ddots & \\vdots \\\\\n",
    "\\dfrac{\\partial^2{\\vec{f}(\\vec{x})}}{\\partial x_{N-1} \\partial x_0} & \\ldots & \\dfrac{\\partial^2{\\vec{f}(\\vec{x})}}{\\partial x_{N-1}^2} \\\\  \n",
    "\\end{array} \\right] ,\n",
    "\\qquad \\text{where } \n",
    "\\vec{H}(\\vec{x})[i,j] = \\dfrac{\\partial^2{\\vec{f}(\\vec{x})}}{\\partial x_i \\partial x_j}.\n",
    "$$\n",
    "\n",
    "(Here again, I have departed from the standard convention by indexing from 0 to $N-1$ instead of from 1 to $N$.)\n",
    "\n",
    "If the second derivative is continuous, which is usually the case in finance applications, the Hessian matrix is symmetric.  This happens because the order in which partial derivatives are taken does not affect the result, so $\\partial^2 f/ \\partial x_i \\partial x_j = \\partial^2 f / \\partial x_j \\partial x_i$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Taylor series\n",
    "\n",
    "The **Taylor series** $T(x)$ of an **infinitely differentiable** function $f(x)$, evaluated at $x$, is the **infinite series** in $\\Delta x$ defined by\n",
    "\n",
    "$$\n",
    "T(x + \\Delta x) := f(x) + f'(x) \\sm \\Delta x + \\frac{1}{2} \\sm f''(x) \\sm \\Delta x^2 + \\ldots \n",
    "+ \\frac{1}{n!} \\sm  f^{(n)}(x) \\sm \\Delta x^n + \\dots ,\n",
    "$$\n",
    "\n",
    "where $f^{(n)}(x)$ denotes the $n\\!$ th derivative of $f$.\n",
    "\n",
    "If you mechanically differentiate $T(x)$ following the rules for differentiating polynomials, you find that each derivative of $T(x)$ is the same as the corresponding derivative of $f(x)$.\n",
    "\n",
    "An infinite series has a **radius of convergence**, defined as the largest value $r$ for which the series convergerges for any $|\\Delta x| < r$. The radius of convergence can be zero, infinity, or anything in between. If the radius of convergence is infinite, the function is called an **entire function**. Entire functions play a central role in **complex analysis**.\n",
    "\n",
    "Here are some examples:\n",
    "\n",
    "1. The Taylor series of a polynomial is the polynomial itself, which has an infinite radius of convergence.\n",
    "\n",
    "2. The **exponential function** $\\e^x$ is defined by its Taylor series:\n",
    "\n",
    "$$\n",
    "\\e^x := 1 + x + \\frac{1}{2} \\sm x^2 + \\ldots + \\frac{1}{n!} \\sm  x^n + \\ldots .\n",
    "$$\n",
    "\n",
    "The exponential function has an infinite radius of convergence. It can also be defined as the function whose derivative is the function itself and satisfies $f(0) = 1$.\n",
    "\n",
    "3. The function $\\log(x)$ has a Taylor series with radius of convergence $|\\Delta x| < x$:\n",
    "\n",
    "$$\n",
    "\\log(x + \\Delta x) = \\log(x) + \\frac{1}{x} \\sm \\Delta x - \\frac{1}{2 \\sm x^2} \\sm \\Delta x^2 + \\ldots + \\frac{(-1)^{n+1}}{n \\sm a^n} \\sm \\Delta x^n + \\ldots,\n",
    "\\qquad \\text{where} \\qquad |\\Delta x| < x>.  \n",
    "$$\n",
    "\n",
    "4. The Taylor series of $f(x) = \\frac{x}{x - \\Delta x}$, expanded around $x=1$, has radius of convergence of 1, and gives us\n",
    "\n",
    "$$\n",
    "\\frac{1}{1 - \\Delta x} = 1 + \\Delta x + \\Delta x^2 + \\dots + \\Delta x^n + \\ldots,\n",
    "\\qquad \\text{where} \\qquad |\\Delta x| < 1.\n",
    "$$\n",
    "\n",
    "The first two terms in a Taylor series, $f(x) + f'(x) \\sm \\Delta x$, define the tangent line approximation to a differentiable function. We can call this a **first-order** approximation (i.e., based on the first derivative only). \n",
    "\n",
    "The first three terms, $f(x) + f'(x) \\sm \\Delta x + \\frac{1}{2} \\sm f''(x) \\sm \\Delta x^2 $ approximate a differentiable function with a quadratic function.   We can call this a **second-order** approximation (i.e., based on the first two derivatives only). This quadratic approximation matches the value (level), first derivative, and second derivative of the approximated function at the point of approximation $x$.\n",
    "\n",
    "In finance, Taylor series arise in studying the time value of money over infinite horizons. Taylors series also arise in equation solving and optimization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional: Implicit function theorem and Inverse function theorem\n",
    "\n",
    "Let $f(x, y)$ be a continuously differentiable function of both $x$ and $y$, where both $x$ and $y$ are real numbers. Consider the following intuition: The equation $f(x, y) = 0$ seems to implicitly define a function $x = g(y)$. Furthermore, I can calculate the derivative of this function by expanding $f(x, y) = 0$ with a Taylor series and solve for $g'(x) = \\drm x / \\drm y$ to obtain\n",
    "\n",
    "$$\n",
    "f(x + \\Delta x, y + \\Delta y) = f(x, y) + \\frac{\\partial f}{\\partial x}(x,y) \\sm \\Delta x \n",
    "+  \\frac{\\partial f}{\\partial y}(x,y) \\sm \\Delta y = 0\n",
    "\\qquad \\text{which implies} \\qquad \n",
    "g'(y) = \\frac{\\drm x}{\\drm y} = -\\frac{\\frac{\\partial f}{\\partial y}(x,y)}{\\frac{\\partial f}{\\partial x}(x,y)} .\n",
    "$$\n",
    "\n",
    "Obviously, this intuition is not always valid. For example, it does not work when $\\frac{\\partial f}{\\partial x}(x,y) = 0$ since this leads to division by zero.\n",
    "\n",
    "The **implicit function theorem** provides conditions under which this intuition is valid: I $f(x, y)$ is continuously differentiable and the partial derivative $\\frac{\\partial f}{\\partial x}(x,y)$ is non-zero, then the function $x = g(y)$ is uniquely defined and continuously differentialbe in a neighborhood of $y$, and its derivative is indeed $g'(y) = -\\frac{\\frac{\\partial f}{\\partial y}(x,y)}{\\frac{\\partial f}{\\partial x}(x,y)}$.\n",
    "\n",
    "Furthermore, the **implicit function theorem** generalizes to vector-valued functions.  Let $\\vec{x}$ and $\\vec{y}$ be vectors in $\\mathbb{R}^M$ and $\\mathbb{R}^N$ respectively. Suppose $f(\\vec{x}, \\vec{y}) : \\mathbb{R}^{M + N} \\rightarrow \\mathbb{R}^{M}$ is continuously differentiable with $(N + M) \\times M$ Jacobian matrix which we write as\n",
    "\n",
    "$$\n",
    "Jf(\\vec{x}, \\vec{y}) := \\left[ Jf_\\vec{x}(\\vec{x}, \\vec{y}), Jf_\\vec{y}(\\vec{x}, \\vec{y}) \\right],\n",
    "$$\n",
    "\n",
    "where $Jf_\\vec{x}(\\vec{x}, \\vec{y})$ and $Jf_\\vec{y}(\\vec{x}, \\vec{y})$ are matrices of partial derivatives.\n",
    "\n",
    "Assume that the $M \\times M$ Jacobian matrix $\\frac{\\partial f}{\\partial \\vec{y}}(\\vec{x}, \\vec{y})$ is invertible. Then $f(\\vec{x}, \\vec{y}) = \\vec{0}_M$ implicitly defines a unique function $g: \\mathbb{R}^M \\rightarrow \\mathbb{R}^M$, whose derivative is \n",
    "\n",
    "$$\n",
    "\\nabla g(\\vec{y}) := -\\left( \\frac{\\partial f}{\\partial \\vec{y}} \\right)^{-1} \\mm \\frac{\\partial f}{\\partial \\vec{x}}.\n",
    "$$\n",
    "\n",
    "The **inverse function theorem** is obtained by assuming $M = N$ and changing $f(\\vec{x}, \\vec{y})$ to $f(\\vec{x}) - \\vec{y}$. Then the implicit function $f(\\vec{x}) = \\vec{0}_N$ has an inverse $\\vec{x} = g(\\vec{y})$ with derivative $(\\frac{\\partial f}{\\partial \\vec{y}})^{-1}$ when the Jacobian matrix $\\frac{\\partial f}{\\partial \\vec{y}}$ is invertible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concave and convex functions\n",
    "\n",
    "A **convex set** is defined by the property that all points on a line segment connecting any two points in the set also lie in the set. Let $X$ be a convex set, and let $f:X \\rightarrow \\mathbb{R}$ be a function.  The function $f(x)$ is a **convex function** if all points on every line segment (**chord** defined by the **secant line**) connecting two points on the graph of the function lie on or above the graph of the function. Mathematically, $f(x)$ is a convex function if, for any two distinct points $x_0$ and $x_1$ and any values of $\\alpha$ such that $0 < \\alpha < 1$, we have\n",
    "\n",
    "$$\n",
    "f \\big( \\alpha \\sm x_0 + (1-\\alpha) \\sm x_1 \\big) \\le \\alpha \\sm f(x_0) + (1 - \\alpha) \\sm f(x_1) .\n",
    "$$\n",
    "\n",
    "A function is **concave** if $-f(x)$ is convex. A function is **strictly convex** if the inequality above is made strict. A linear function is both concave and convex but is neither strictly concave nor strictly convex.\n",
    "\n",
    "Examples of convex functions are the quadratic function $f(x) = \\frac{1}{2} \\sm a \\sm x^2 + b \\sm x + c$ with $a>0$ and the exponential function $\\e^x$. Examples of concave functions are the same quadratic function, except with $a<0$, and the function $\\log(x)$.\n",
    "\n",
    "Here is a set of plots illustrating convexity and concavity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x400 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "nobs = 1000\n",
    "x0, x1 = -1.00, +2.00\n",
    "x = np.linspace(start=x0, stop=x1, num=nobs, endpoint=True)\n",
    "a, b, c = 1.00, 0.50, -1.00\n",
    "fs = [lambda x: a*x**2 + b*x + c, np.exp, lambda x: -a*x**2 + b*x + c, lambda x: np.log(2.00 + x)]\n",
    "y = [f(x) for f in fs]\n",
    "\n",
    "xx0, xx1 = -0.50, 1.50\n",
    "xx = np.linspace(start=xx0, stop=xx1, num=nobs, endpoint=True)\n",
    "alphas = np.linspace(start=0.00, stop=1.00, num=nobs, endpoint=True)\n",
    "yy = [(1 - alphas) * f(xx0) + alphas * f(xx1) for f in fs]\n",
    "\n",
    "fig, ax = plt.subplots(1, 4, figsize=(10, 4))\n",
    "\n",
    "titles = [\"Quadratic: convex\", \"$\\exp$: convex\", \"Quadratic: concave\", \"$\\log$: concave\"]\n",
    "for i in range(4):\n",
    "\n",
    "    ax[i].scatter(x, y[i] , color='b', s=1.0)\n",
    "    ax[i].scatter(xx, yy[i] , color='r', s=1.0)\n",
    "    ax[i].set_title(titles[i])\n",
    "    ax[i].set_xlabel(\"$x$\")\n",
    "    ax[i].set_ylabel(\"$y$\")\n",
    "\n",
    "plt.tight_layout()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jensen's inequality\n",
    "\n",
    "**Jensen's inequality** states that if $f(x)$ is a convex function and $X$ is a random variable defined on the convex domain of $f$, then then\n",
    "\n",
    "$$\n",
    "f \\left( \\E [ X ] \\right) \\le \\E \\left[ f(X) \\right].\n",
    "$$\n",
    "\n",
    "Jensen's inequality generalizes the idea that the red line line segment lies above the blue graph of the functions $f(x)$ in the plots above. For example, if $X$ is a probability distribution equal to $x_0$ with probability $\\alpha$ and $x_1$ with probability $1-\\alpha$, then blue line is the left side of the above inequality and the red line is the right side, with different points on the blue and red lines corresponding to different values of $\\alpha$.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convexity and derivatives\n",
    "\n",
    "If a univariate convex function $f(x)$ is differentiable in a convex domain $\\mathcal{D}$, the graph of the function lies above or on every tangent line:\n",
    "\n",
    "$$\n",
    "f(x + \\Delta x) \\ge f(x) + f'(x) \\sm \\Delta x \\qquad \\text{for all} \\qquad x + \\Delta x \\in \\mathcal{D}.\n",
    "$$\n",
    "\n",
    "This idea generalizes to multiple dimensions.  If $\\vec{f}(\\vec{x})$ is a differentiable univariate function of a vector $\\vec{x}$ on a convex vector domain $\\mathcal{D}$, and $\\Delta \\vec{x}$ is another vector, then\n",
    "\n",
    "$$\n",
    "\\vec{f}(\\vec{x} + \\Delta{\\vec{x}}) \\ge \\vec{f}(\\vec{x}) + \\nabla\\vec{f}(\\vec{x}) \\mm \\Delta \\vec{x}\n",
    " \\qquad \\text{for all} \\qquad \\vec{x} + \\Delta \\vec{x} \\in \\mathcal{D} .\n",
    "$$\n",
    "\n",
    "For this vector case, the tangent plane or hyperplane associated with the gradient $\\nabla \\vec{f}(\\vec{x})$ lies below or on the multidimensional graph of the function.\n",
    "\n",
    "A univariate function with both first and second derivatives is convex if and only if its second derivative is nonnegative everywhere. It is concave if and only if its second derivative is nonpositive everywhere.\n",
    "\n",
    "To generalize this idea to functions with vector arguments, we need to define what it means for the hessian matrix to be \"nonnegative\".  The correct generalization uses the concept of a **positive semidefinite matrix**. If a function mapping vectors to scalars, from a convex doman, has a hessian matrix which is itself a continuous function, the function is convex if and only if its hessian matrix is positive semidefinite for all values of $\\vec{x}$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimization problems\n",
    "\n",
    "In finance and economics, it is often important to find the maximum or mininum value of a function on a specific domain.\n",
    "\n",
    "The field of **mathematical optimization** considers many different types of problems. It is an active field of research, with new research problems opened up by the increasing power of computers.\n",
    "\n",
    "Well-formulated optimization problems are often called **programs** (not to be confused with **computer programs**); thus, mathematical optimization is often called **mathematical programming**. Often, there are constraints.  \n",
    "\n",
    "Optimization problems can be classified as **discrete** or **continuous**. **Integer programming** or **combinatorial optimization** solves problems for optimal values of parameters in finite or countable sets, typically represented as `int32` or `int64`. **Continuous optimization** solves problems over continuous variables, typically `float32` or `float64` approximations to real numbers.\n",
    "\n",
    "The **standard form** for a generic  **constrained optimization problem** is to minimize a function with both inequality constraints and equality constraints:\n",
    "\n",
    "$$\n",
    " \\min_{\\vec{x}}  f(\\vec{x})  \n",
    " \\qquad \\text{subject to} \\qquad \n",
    " g_j(\\vec{x}) \\le 0  \\text{ for } j = 0, \\ldots, J-1,\n",
    " \\qquad h_k(\\vec{x}) = 0 \\text{ for } k = 0, \\ldots, K-1.\n",
    "$$\n",
    "\n",
    "Here $f$, $g_j$, and $h_k$ are functions mapping $\\mathbb{R}^N$ into $\\mathbb{R}$. The function $f(\\vec{x})$ is called the **objective function**, the functions $g_j(\\vec{x})$ define **inequality constraints**, and the functions $h_k(\\vec{x})$ define **equality constraints**. If there are no constraints ($J=K=0$), the problem is an **unconstrained optimization problem**.\n",
    "\n",
    "The optimization problem could be equivalently written as a maximization problem by replacing $f$ with $-f$. Statisticians often formulate optimization problems as minimization problems. In finance and economics, optimization problems are typically formulated as maximization problems (e.g., maximizing profits, maximizing utility, or---for econometric applications---maximum likelihood).\n",
    "\n",
    "Optimization problems can also be classified into **convex** or **nonconvex**. **Convex programming**  solves problems where, in the standard form above, the function $f$ is convex, the functions $g_j$ are all convex, and the functions $h_k$ are linear.  Nonconvex programming solves problems where these functions need need not have these convexity properties.\n",
    "\n",
    "**Linear programming** solves problems in which $f$, $g_j$, and $h_k$ are all linear. **Quadratic programming** solves problems in which $f$ is quadratic, the $g_j$ are all linear, and the $h_k$ are all linear.  If you add a quadratic form to the objective function in a linear program, you obtain a quadratic program."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First- and second-order conditions for optimization\n",
    "\n",
    "We will be concerned with continuous optimization. In general, the difficulty of finding an optimum (either maximum or minimum) depends on whether a function is continuous, whether it has a first derivative, whether it has a second derivative, whether the derivatives are \"well-behaved\", and whether it is convex or concave.\n",
    "\n",
    "For an arbitrary function $f(x)$ which is not continuous, finding its minimum is hopelessly difficult. For example, suppose $f(x)=1$ at all points except one point $x_0$, where $f(x_0)=0$. Finding the minimum of this function is equivalent to finding $x_0$. If $x_0$ is some random point, it will generally be impossible to find because there are an uncountably infinite number of real numbers and no algorithm is better than random guessing.\n",
    "\n",
    "Now consider a continuous function $f(\\vec{x})$ from a subset of $\\mathbb{R}^N$ to $\\mathbb{R}$. Suppose its domain is a **compact** set. A set is **compact** if it is  **closed** and **bounded**.  A set is **closed** if any convergent sequence of points converges to a point in the set.  The **open interval** $(0,1)$ is not closed because sequences of points can converge to the endpoints $0$ and $1$, which are not in the set. A set is **bounded** if the distance between any two points is less than some fixed finite value.  The **closed interval** $[0,1]$ is compact because it is both closed and bounded. The **boundary** of the set is the subset of points such that there exist points not in the set which are arbitrarily close to the boundary point. Points not on the boundary are **interior points**.\n",
    "\n",
    "A continuous function defined on a compact set achieves both its maximum and minimum at some point in the set. This is an important and useful fact, but it does not make the problem of finding the maximum or minimum easy. In principle, it may be necessary to conduct an exhaustive **grid search**, but even a grid search may not work. Consider, for example, the problem of an ant finding the tallest tower on the surface of the earth (a sphere is compact).  If the tower is a pencil-thin object located at some random point, the ant might need to search every location the size of the end of a pencil.\n",
    "\n",
    "Now suppose the function has a first derivative at all points in the interior of its domain, which we will assume to be a closed set. If the maximum or minimum of the function is an interior point, the derivative of a function $f:\\mathbb{R} \\rightarrow \\mathbb{R}$ must be zero at that point. This fact is obvious because, if the derivative is nonzero, a higher or lower function value is obtained by moving to a point slightly larger or smaller. For a function $f:\\mathbb{R}^N \\rightarrow \\mathbb{R}$, the gradient at a maximum or minimum must be the zero vector, which I denote $\\vec{0}_N$. We say that such points satisfy the **first-order conditions** for an optimum. The first-order conditions are **necessary**, but not **sufficient**, conditions for an interior point to be an  optimum. If there are only a finite number of points satisfying the first order conditions, and the global optimum is an interior point, then one of the points satisfying the first-order conditions must be a **global optimum**. In general, a point satisfying the first-order conditions may be global maximum, global minimum, **local maximum**, **local minimum**, or neither a local maximum or minimum. If the optimum exists but is not an interior point, then it lies somewhere on the boundary. \n",
    "\n",
    "Now suppose the function has a both first and second derivatives at all points on the interior of its domain. For $f:\\mathbb{R} \\rightarrow \\mathbb{R}$, if $f$ satisfies the first-order condition $f'(x)=0$ and also satisfies the **second-order condition** for a minimum $f''(x)>0$, the point $x_0$ is a local minimum, and may or may not be a global minimum. Analogously, if $f'(x_0)=0$ and $f$ satisfies the **second-order condition** for a local maximum $f''(x_0) < 0$, then the point $x_0$ is a local maximum, and may or may not be a global maximum. \n",
    "\n",
    "This point generalizes to a function defined on a domain of vectors, such as $f:\\mathbb{R}^N \\rightarrow \\mathbb{R}$. Suppose the function satisfies the first-order condition that the gradient is zero, $\\nabla f(\\vec{x}) = \\vec{0}_N$. The second derivative is a square, symmetric hessian matrix $\\vec{H f}(\\vec{x})$. The second-order condition for a maximum generalizes from the univariate derivative being negative to the hessian matrix being **negative definite**. Similarly, the second-order condition for a minimum is that the hessian matrix is **positive definite**. These second-order conditions are **sufficient** for a local optimum, but they are not necessary. For example, when the hessian is neither positive definite nor negative definite, it might be positive semidefinite or negative semidefinite, in which case a point might be a local maximum, local minimum, or neither (**saddle point**).\n",
    "\n",
    "Even when a function has second derivatives, the problem or characterizing the maxima and minima of the function is very difficult. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solving optimization problems\n",
    "\n",
    "One way to solve optimization problems is to find all solutions to the first-order conditions then compare all of these solutions and also compare the solutions with values of the function on its boundary. This approach works for simple problems, such as the case when the objective function is a polynomial defined on a closed interval. In this case it is known that the polynomial of degree $N$ has at most $N$ roots, algorithms exist for finding these roots, and the $N$ roots can be compared with the two boundary points to find the global optimum.\n",
    "\n",
    "Often, it is impractical to find all solutions to the first-order conditions and evaluate them.  This is especially the case when $f:\\mathbb{R}^N \\rightarrow \\mathbb{R}$ is a vector-valued function, and the vector has high dimension (i.e., $N$ is a large number). Researchers typically solve such optimization problems, whether constrained or unconstrained, by searching over the domain of the function using an **iterative** approach which starts with a guess and then tries to improve the guess by taking advantage of the known structure of the problem, such as whether first or second derivatives exist and can be calculated in a practical way. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convex optimization\n",
    "\n",
    "The problem of finding an unconstrained minimum is easier if it is known that the function is convex. Analagously, it is equally easier to find an unconstrained maximum if it is known that the function is concave. \n",
    "\n",
    "Here are the main important points:\n",
    "\n",
    "1. If the first-order conditions $\\nabla f(\\vec{x}) = \\vec{0}_N$ are satisfied, the point $\\vec{x}$ is a global minimum, not just a candidate local minimum.\n",
    "\n",
    "2. If the second-order condition that the hessian matrix, $\\vec{H f}(\\vec{x})$, is positive definite is also satisfied, then a point satisfying the first-order conditions is the **unique** global minimum, not just a local minimum.\n",
    "\n",
    "3. If the hessian matrix, $\\vec{H f}(\\vec{x})$, is positive definite at every point in the domain of the function, then the function is globally strictly convex.\n",
    "\n",
    "In finance, economics, statistics, and machine learning applications, it is sometimes known that a function is concave or convex. Sometimes it is not known whether an objective function is concave or convex. When it is not known, it is typical for an algorithm to find a point which can be verified to be a local minimum, due to satisfying first- and second-order conditions. Typically, a proof that the point is a globabl minimum is elusive. A researcher can try to find an improved local minimum by running the algorithm again from different starting points or by randomly perturbing known local minima to see if a better local minimum is found. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unconstrained quadratic optimization problems\n",
    "\n",
    "Let us consider an unconstrained optimization problem with a quadratic objective function.  In principle, this should be an easy problem. In fact, some interesting algorithmic issues arise which make solving such problems harder than you might think.\n",
    "\n",
    "The univariate case is indeed easy. Let $f(x) : \\mathbb{R} \\rightarrow \\mathbb{R}$ be a univariate quadratic function defined by $f(x) = \\frac{1}{2} \\sm a \\sm x^2  + b \\sm x + c$, where $a$, $b$, and $c$ are scalars. The first derivative is the linear function $f'(x) = a \\sm x + b$, and the second derivative is the constant function $f''(x) = a$. The solution to the first-order condition $f'(x) = 0$ is $x = -b/a$. If $a$ is positive, the function is globally convex. The second-order conditions for a minimum is satisfied, so $x = -b/a$ is the global minimum.  Analoguously, if $a$ is strictly negative, the solution to the first-order condition is a global maximum.\n",
    "\n",
    "Now let us generalize the univariate case to the case of a quadratic vector-valued function $f:\\mathbb{R}^N \\rightarrow \\mathbb{R}$. This function has the form\n",
    "\n",
    "$$\n",
    "f(\\vec{x}) = \\tfrac{1}{2} \\sm \\vec{x} \\t \\mm \\vec{A} \\mm \\vec{x} + \\vec{b} \\t \\mm \\vec{x} + c. \n",
    "$$\n",
    "\n",
    "Here $\\vec{A}$ is a symmetric $N \\times N$ matrix, which we will assume to be positive definite, consistent with the second-order condition for a minimization problem; $\\vec{b}$, like $\\vec{x}$, is a vector of size $N$; and $c$ is a scalar. The term $ \\tfrac{1}{2} \\sm \\vec{x} \\t \\mm \\vec{A} \\mm \\vec{x}$ is a **quadratic form**, which is a positive scalar if $\\vec{x} \\ne \\vec{0}_N$ (since $\\vec{A}$ is positive definite). The gradient vector is the **linear functional**\n",
    "\n",
    "$$\n",
    "\\nabla f(\\vec{x}) =  \\vec{A} \\mm \\vec{x}  + \\vec{b}. \n",
    "$$\n",
    "\n",
    "If $\\vec{A}$ were not symmetric, the derivative of $\\vec{x} \\t \\mm \\vec{A} \\mm \\vec{x}$ would have been $\\tfrac{1}{2} \\sm (\\vec{A} + \\vec{A} \\t) \\mm \\vec{x}$, but this is equal to $\\vec{A} \\mm \\vec{x}$ since $\\vec{A}$ is assumed to be symmetric. \n",
    "\n",
    "If we type this notation into Python, using Numpy, we get what we probably want and expect:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A.shape=(2, 2)\n",
      "b.shape=(2,)\n",
      "type(c)=<class 'float'>\n",
      "x.shape=(2,)\n",
      "\n",
      "(x.T @ A).shape=(2,)\n",
      "(A @ x).shape=(2,)\n",
      "type(x.T @ A @ x)=<class 'numpy.float64'>\n",
      "type(b @ x)=<class 'numpy.float64'>\n",
      "type(res)=<class 'numpy.float64'>\n",
      "(x @ A).shape=(2,)\n",
      "(A @ x.T).shape=(2,)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "A = np.array([2.0, 0.0, 0.0, 4.0]).reshape((2,2))\n",
    "b = np.array([3.0, 5.0])\n",
    "c = 7.0\n",
    "x = np.array([0.00, 0.0])\n",
    "\n",
    "def f(x):\n",
    "    return  0.50 * x.T @ A @ x + b @ x + c\n",
    "\n",
    "res = f(x)\n",
    "\n",
    "print(f\"{A.shape=}\\n{b.shape=}\\n{type(c)=}\\n{x.shape=}\\n\")\n",
    "print(f\"{(x.T @ A).shape=}\\n{(A @ x).shape=}\\n{type(x.T @ A @ x)=}\\n{type(b @ x)=}\\n{type(res)=}\")\n",
    "print(f\"{(x @ A).shape=}\\n{(A @ x.T).shape=}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note on Notation\n",
    "\n",
    "In the Python cell above, the vectors `b` and `x` are defined as one-dimensional arrays of shape `(2,)`. This means that `b` and `x` are \"generic\" vectors, which are neither row vectors nor column vectors.  To obtain column vectors, we could have defined `b` and `x` to have shape `(2,1)` by using `reshape((2,1))`.\n",
    "\n",
    "The mathematical gradient $\\nabla f(\\vec{x}) =  \\vec{A} \\mm \\vec{x}  + \\vec{b}$ implicitly assumes that the gradient vector is a column vector. This is inconsistent with the convention that the gradient vector is a row vector. To make the gradient a row vector, we should have written\n",
    "\n",
    "$$\n",
    "\\nabla f(\\vec{x}) \\t =  \\vec{A} \\mm \\vec{x}  + \\vec{b}\n",
    "\\qquad \\text{or} \\qquad\n",
    "\\nabla f(\\vec{x}) =  \\vec{x} \\t \\mm \\vec{A} \\t  + \\vec{b} \\t,\n",
    "\\qquad \\text{not} \\qquad\n",
    "\\nabla f(\\vec{x}) =  \\vec{A} \\mm \\vec{x}  + \\vec{b}.\n",
    "$$\n",
    "\n",
    "Using one-dimensional Numpy arrays is a convenient way to avoid having to make distinction between column vectors and row vectors when such a distinction is unnecessary and confusing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution to quadratic problem\n",
    "\n",
    "The first-order condition for the quadratic problem sets the gradient $\\nabla f(\\vec{x}) =  \\vec{A} \\mm \\vec{x}  + \\vec{b}$ to zero:\n",
    "\n",
    "$$\n",
    " \\vec{A} \\mm \\vec{x}  + \\vec{b} = \\vec{0}_N.\n",
    "$$\n",
    "\n",
    "Since the hessian is the matrix $\\vec{A}$, which is assumed to be positive definite, solving the linear equation above for $\\vec{x}$ provide the unique global minimum value of $\\vec{x}$ we are looking for.\n",
    "\n",
    "A formal mathematical solution is given by \n",
    "\n",
    "$$\n",
    "\\vec{x} = -\\vec{A}^{-1} \\mm \\vec{b},\n",
    "$$\n",
    "\n",
    "where $\\vec{A}$ is the matrix inverse. This is the natural generalization of the scalar solution $x = -b / a$, which could have been written $x = -\\dfrac{1}{a} \\sm b$.\n",
    "\n",
    "Since $\\vec{A}$ is assumed to be symmetric and positive definite, this linear equation has a special structure which makes finding its solution numerically easier than a problem with an arbitrary matrix $\\vec{A}$.\n",
    "\n",
    "There are many ways to solve linear equations with Python using Numpy or Scipy:\n",
    "\n",
    "1.  **Cholesky decomposition**: This algorithm only works for square matrices which are symmetric and positive definite. Since Cholesky decomposition is fast, stable, and typically very accurate, this is the best algorithm when $\\vec{A}$ is positive definite. If you want to test whether a matrix $\\vec{A}$ is positive definite, try a Cholesky decomposition. If it succeeds, the matrix is positive definite; if it fails, the matrix is (numerically) not positive definite. With Python, a solution to the linear system using Cholesky decomposition can be implemented with  `x = np.linalg.solve(A, -b, assume_a='pos')`.  The option `assume_a='pos'` tells the algorithm to use a Cholesky decomposition.\n",
    "\n",
    "2. **Singular value decomposition** (**SVD**): This algorithm is slow but very stable and very accurate. It works for singular matrices of arbitrary shape. If Cholesky decomposition fails because the matrix is numerically singular, I recommend using a singular value decomposition because (1) the algorithm is more likely to succeed than alternatives and (2) it automatically diagnoses how the matrix is singular.  With Python, the SVD is obtained by `x, residuals, rank, s = np.linalg.lstsq(A, -b, rcond=-1)`.\n",
    "\n",
    "3. **QR decomposition**: This algorithm is slower than Cholesky decomposition, it works for arbitrary matrices, and it is usually very accurate. It can give a good solution to the problem. I recommend Cholesky decomposition because it is about four times faster and usually at least as accurate. If Cholesky decomposition fails, I recommend SVD over QR decomposition because SVD is likely to be somewhat more accurate, somewhat more stable, and even better able to diagnose singularities.  The advantage of QR decomposition over SVD is that QR is several times faster.\n",
    "\n",
    "4. **LU decomposition**: This algorithm is slower than Cholesky decomposition by about a factor of two, it is sometimes less accurate than Cholesky decomposition, and it is less stable than Cholesky decomposition. It is about 2X faster but less accurate and less stable than QR decomposition. I recommend using it if $\\vec{A}$ is not positive definite or not symmetric, you need speed, but you are willing to sacrifice accuracy. LU decomposition can be implemented with  `x = np.linalg.solve(A, -b, assume_a='gen')`.  The option `assume_a='gen'` tells the algorithm to use an LU decomposition, which works for invertible matrices which are not symmetric and not positive definite; the option `assume_a='sym'` tells the function to use a modified LU decomposition appropriate for a symmetric system which is not necessarily positive definite.\n",
    "\n",
    "5. **Iterative methods** (such as the **conjugate gradient method**): These methods are appropriate when the matrix is very large, can be efficiently written as a sparse matrix (i.e., a matrix with many zeros), and is not too poorly conditioned. In Python, the conjugate gradient method is obtained by `scipy.sparse.linalg.cg(A, -b )`.\n",
    "\n",
    "6. **Explicit inverse**: Using an explicit inverse is usually not a good way to solve linear systems because numerical error can be large, and algorithms are slow. \n",
    "\n",
    "7. **Moore-Penrose Pseudoinverse**: This is a variation on SVD. It is better than an explicit inverse if the matrix is close to singular.  It can be obtained with `np.linalg.pinv` or `scipy.linalg.pinv`. In general, `pn.linalg.lstsq` is recommended over `np.linalg.pinv` unless you need to solve many least squares problems with the same singular value decomposition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examples of usage of various algorithms\n",
    "\n",
    "The example in the following cell illustrate usage of various Python functions for solving linear equations.\n",
    "\n",
    "The matrix $A$ is symmetric since it is obtained as a transpose product `C.T @ C`. It is singular if `n < m` and `epsilon == 0.0`. It is positive definite if `epsilon > 0.0`.\n",
    "\n",
    "Google the various functions for matrix decompositions to learn more. This topic is discussed further in the Background Note on Least Squares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "np.linalg.norm(error_chol, ord=np.inf)=9.75108882528275e-13\n",
      "np.linalg.norm(error_sym, ord=np.inf)=9.490186414495838e-13\n",
      "np.linalg.norm(error_lu, ord=np.inf)=3.1437075165285933e-12\n",
      "np.linalg.norm(error_lstsq_gelss, ord=np.inf)=4.894334937333156e-12\n",
      "np.linalg.norm(error_lstsq_gelsd, ord=np.inf)=1.5077002146757223e-11\n",
      "np.linalg.norm(error_lstsq_gelsy, ord=np.inf)=2.6139646003287e-12\n",
      "np.linalg.norm(error_svd_gesdd, ord=np.inf)=4.894334937333156e-12\n",
      "np.linalg.norm(error_svd_gesvd, ord=np.inf)=5.34908228821962e-12\n",
      "np.linalg.norm(error_qr, ord=np.inf)=1.9123869154924478e-12\n",
      "np.linalg.norm(error_qrp, ord=np.inf)=2.418953926053291e-12\n",
      "np.linalg.norm(error_cg, ord=np.inf)=1.7690293674377244e-12\n",
      "i=0, abs_error_norm=3.4404464875098873e-12, rel_error_norm=6.011555129537264e-13, np.linalg.norm(error, ord=np.inf)=1.403765992336048e-12\n",
      "i=1, abs_error_norm=4.534229648616861e-12, rel_error_norm=7.92274247008305e-13, np.linalg.norm(error, ord=np.inf)=3.5984548674150574e-12\n",
      "i=2, abs_error_norm=3.299706027467603e-12, rel_error_norm=5.765636747265609e-13, np.linalg.norm(error, ord=np.inf)=1.4442613771592505e-12\n",
      "i=3, abs_error_norm=2.67553599208469e-12, rel_error_norm=4.675013018185207e-13, np.linalg.norm(error, ord=np.inf)=1.0149658891123181e-12\n",
      "i=4, abs_error_norm=2.9272185451439095e-12, rel_error_norm=5.114782550526709e-13, np.linalg.norm(error, ord=np.inf)=1.2842227281595342e-12\n"
     ]
    }
   ],
   "source": [
    "# Small positive values of epsilon make A positive definite but almost singular:\n",
    "epsilon = 1e-2\n",
    "\n",
    "# The singular value decomposition ignores singular values smaller than cond_tol:\n",
    "cond_tol = 1e-14\n",
    "\n",
    "# Choosing n < m  and epsilon=0.0 makes the matrix A singular:\n",
    "n = 40\n",
    "m = 50\n",
    "\n",
    "rng = np.random.default_rng(seed=1234)\n",
    "C = rng.standard_normal((n,m))\n",
    "A = C.T @ C + epsilon * np.eye(m)\n",
    "b = rng.standard_normal((m,))\n",
    "\n",
    "def finv(x, cond=1e-14):\n",
    "    \"\"\"\n",
    "    x = numpy array of arbitrary shape\n",
    "    cond = scalar cutoff value\n",
    "\n",
    "    res = numpy array the same shape as x. \n",
    "    If the absolute value of an element is greater than or equal to `cond`, return the reciprocal of the element.\n",
    "    If the absolute value of an element is less than `cond`, return zero.\n",
    "\n",
    "    This function is used to set the reciprocal of small singular values in the singular value decomposition to zero.\n",
    "    \"\"\"\n",
    "    res = np.divide(1.00, x, out = np.zeros_like(x), where=(np.abs(x) / x.max() >= cond))\n",
    "    return res\n",
    "\n",
    "# Various algorithms for solving the linear system A @ x = b:\n",
    "\n",
    "# cholesky decomposition:\n",
    "try:\n",
    "    x_chol = scipy.linalg.solve(A, b, assume_a='pos')\n",
    "    error_chol = A @ x_chol - b\n",
    "    print(f\"{np.linalg.norm(error_chol, ord=np.inf)=}\")\n",
    "except:\n",
    "    print(f\"solve pos = chol cholesky failed\")\n",
    "\n",
    "# symmetric LU decomposition:\n",
    "try:\n",
    "    x_sym = scipy.linalg.solve(A, b, assume_a='sym')\n",
    "    error_sym = A @ x_sym - b\n",
    "    print(f\"{np.linalg.norm(error_sym, ord=np.inf)=}\")\n",
    "except:\n",
    "    print(f\"solve sym failed\")\n",
    "\n",
    "# generic LU decomposition:\n",
    "try:\n",
    "    x_lu =scipy.linalg.solve(A, b, assume_a='gen')\n",
    "    error_lu = A @ x_lu - b\n",
    "    print(f\"{np.linalg.norm(error_lu, ord=np.inf)=}\")\n",
    "except:\n",
    "    print(f\"solve gen failed\")\n",
    "\n",
    "# SVD from scipy.linalg.sltsq using three different algorithms (lapack_driver=...), which should give same result:\n",
    "try:\n",
    "    x_lstsq_gelss, residuals, rank, s = scipy.linalg.lstsq(A, b, cond=None,lapack_driver='gelss')\n",
    "    error_lstsq_gelss = A @ x_lstsq_gelss - b\n",
    "    print(f\"{np.linalg.norm(error_lstsq_gelss, ord=np.inf)=}\")\n",
    "except:\n",
    "    print(f\"lstsq_gelss = svd failed\")\n",
    "\n",
    "try:\n",
    "    x_lstsq_gelsd, residuals, rank, s = scipy.linalg.lstsq(A, b, cond=None, lapack_driver='gelsd')\n",
    "    error_lstsq_gelsd = A @ x_lstsq_gelsd - b\n",
    "    print(f\"{np.linalg.norm(error_lstsq_gelsd, ord=np.inf)=}\")\n",
    "except:\n",
    "    print(f\"lstsq_gelsd failed\")\n",
    "\n",
    "try:\n",
    "    x_lstsq_gelsy, residuals, rank, s = scipy.linalg.lstsq(A, b, cond=None,lapack_driver='gelsy')\n",
    "    error_lstsq_gelsy = A @ x_lstsq_gelsy - b\n",
    "    print(f\"{np.linalg.norm(error_lstsq_gelsy, ord=np.inf)=}\")\n",
    "except ():\n",
    "    print(f\"lst_gelsy  svd failed\")\n",
    "\n",
    "# Direct use of SVD which requires two steps, using two different algorithms:\n",
    "try:\n",
    "    u_svd_gesdd, s_svd_gesdd, vt_svd_gesdd = scipy.linalg.svd(A, lapack_driver='gesdd')\n",
    "    x_svd_gesdd = vt_svd_gesdd.T @ (finv(s_svd_gesdd, cond=cond_tol) * (u_svd_gesdd.T @ b))\n",
    "    error_svd_gesdd = A @ x_svd_gesdd - b\n",
    "    print(f\"{np.linalg.norm(error_svd_gesdd, ord=np.inf)=}\")\n",
    "except ():\n",
    "    print(f\"svd_gesdd failed\")\n",
    "\n",
    "try:\n",
    "    u_svd_gesvd, s_svd_gesvd, vt_svd_gesvd = scipy.linalg.svd(A, lapack_driver='gesvd')\n",
    "    x_svd_gesvd = vt_svd_gesvd.T @ (finv(s_svd_gesvd, cond=cond_tol) * (u_svd_gesvd.T @ b))\n",
    "    error_svd_gesvd = A @ x_svd_gesvd - b\n",
    "    print(f\"{np.linalg.norm(error_svd_gesvd, ord=np.inf)=}\")\n",
    "except:\n",
    "    print(f\"svd_gesvd failed\")\n",
    "    print(f\"{s_svd_gesvd=}\")\n",
    "\n",
    "# Two examples of QR decomposition using two different algorithms (with and without pivoting):\n",
    "try:\n",
    "    q, r  = scipy.linalg.qr(A, mode='economic', pivoting=False)\n",
    "    x_qr = scipy.linalg.solve_triangular(r, q.T.dot(b))\n",
    "    error_qr = A @ x_qr - b\n",
    "    print(f\"{np.linalg.norm(error_qr, ord=np.inf)=}\")\n",
    "except ():\n",
    "    print(f\"qr failed\")\n",
    "\n",
    "try:\n",
    "    q, r, p  = scipy.linalg.qr(A, mode='economic', pivoting=True)\n",
    "    x_qrp = scipy.linalg.solve_triangular(r, q.T.dot(b))\n",
    "    error_qrp = A[:, p] @ x_qrp - b\n",
    "    print(f\"{np.linalg.norm(error_qrp, ord=np.inf)=}\")\n",
    "except ():\n",
    "    print(f\"qrp pivot failed\")\n",
    "\n",
    "# conjugate gradient method:\n",
    "try:\n",
    "    x_cg, infocg = scipy.sparse.linalg.cg(A, b, rtol=1e-12, maxiter=1000 )\n",
    "    error_cg = A @ x_cg - b\n",
    "    print(f\"{np.linalg.norm(error_cg, ord=np.inf)=}\")\n",
    "except:\n",
    "    print(f\"cg failed\")\n",
    "\n",
    "# Illustration of iterative refinement, which is not needed if original solution is accurate enough:\n",
    "try:\n",
    "    x = x_svd_gesvd\n",
    "    error = A @ x - b\n",
    "    for i in range(5):\n",
    "        dx = vt_svd_gesvd.T @ (finv(s_svd_gesvd, cond=cond_tol) * (u_svd_gesvd.T @ error))\n",
    "        x = x - dx\n",
    "        error = A @ x - b\n",
    "        abs_error_norm = np.linalg.norm(error, ord=2)\n",
    "        rel_error_norm = np.linalg.norm(error, ord=2) / np.linalg.norm(b, ord=2)\n",
    "        print(f\"{i=}, {abs_error_norm=}, {rel_error_norm=}, {np.linalg.norm(error, ord=np.inf)=}\")\n",
    "except:\n",
    "    print(\"iterative refinement failed\")        \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 1 \n",
    "\n",
    "1. (Easy if you read these notes carefully) If you successfully use a Cholesky decomposition to solve the linear system associated with a quadratic problem, how do you deal with the issue of verifying whether the second-order condition holds or not?\n",
    "\n",
    "2. By playing with the parameters in the above cell, try to construct examples to illustrate cases where some of the algorithms for solving linear systems work better than others. (Do not spend too much time on this problem unless you want to.) \n",
    "\n",
    "3. Consider the quadratic function \n",
    "$$\n",
    "f(\\vec{x}) = \\tfrac{1}{2} \\sm \\vec{x} \\t \\mm \\vec{A} \\mm \\vec{x} + \\vec{b} \\t \\mm \\vec{x} + c. \n",
    "$$\n",
    "> > For the $2 \\times 2$ case with symmetric matrix $\\vec{A}$,\n",
    "$$\n",
    "\\vec{A} = \\left[ \\begin{array}{c c}\n",
    "a_{0 0} & a_{0 1} \\\\\n",
    "a_{1 0} & a_{1 1} \\\\\n",
    "\\end{array} \\right],\n",
    "\\qquad\n",
    "\\vec{b} =\\left[ \\begin{array}{c} b_0 \\\\ b_1 \\end{array} \\right],\n",
    "\\qquad\n",
    "\\vec{x} =\\left[ \\begin{array}{c} x_0 \\\\ x_1 \\end{array} \\right],\n",
    "$$\n",
    "> > verify that $\\nabla f(\\vec{x}) = \\vec{A} \\mm \\vec{x} + \\vec{b}$ by differentiating the quadratic function \"by hand\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "u=array([[1., 0.],\n",
      "       [0., 1.]])\n",
      "s=array([1., 1.])\n",
      "v=array([[-1., -0.],\n",
      "       [ 0.,  1.]])\n"
     ]
    }
   ],
   "source": [
    "x = np.array([-1,0,0,1], dtype=np.float64).reshape((2,2))\n",
    "u,s,v = np.linalg.svd(x)\n",
    "print(f\"{u=}\\n{s=}\\n{v=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimization algorithms\n",
    "\n",
    "To maximize or minimize a differentiable function, optimization algorithms typically proceed iteratively by starting with an inititial guess, then calculating the gradient at that point. **Gradient methods** are optimization algorithms which adjust the initial guess by moving in the direction of the gradient or negative-gradient, which can be interpreted as a direction of **steepest ascent** or **steepest descent**. \n",
    "\n",
    "The **stepsize** refers to how large an adjustment to the initial guess is made in the chosen **search direction** (e.g., gradient or negative-gradient).  A typical approch is to use **backtracking**, which shrinks the size of the step until the objective function is improved at the reduce step.\n",
    "\n",
    "A **line search** algorithm chooses the size of the step to take by solving a one-dimensional optimization subproblem. A line search algorithm might try to solve the one-dimensional subproblem almost exactly. \n",
    "\n",
    "A typical optimization algorithm uses the new guess which results from the chosen step size a new starting point for the next iteration of the algorithm.  Iterations continue until further iterations result in very small improvements in the objective function.\n",
    "\n",
    "The generical gradient method described above is called a **first-order method** because it only uses first derivatives.\n",
    "\n",
    "A **second order method** uses both first and second derivatives (i.e., both gradients and hessions). The intuition for a second order method is to approximate the function being optimized with a second-order Taylor series expansion, which has a term based on the gradient and an additional term based on the hessian. Optimizing this quadratic approximation is the same as solving for the point at which the first-order condition is zero. Variations on **Newton's method** (used above) can work well.  Obviously, second-order methods work better if the function is convex, or at least locally convex. \n",
    "\n",
    "Instead of chosing as a search direction the gradient or negative gradient, the second-order method uses as a search direction the solution to the quadratic optimization problem which approximates the actual problem. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First-order versus second-order methods\n",
    "\n",
    "Here are some considerations relevant to choosing between a first-order method and a second-order method:\n",
    "\n",
    "1. First order methods can take many iterations to converge. Often, thousands of iterations are required, with search directions zigzagging up or down the \"hill\" being ascended or descended. Second-order methods converge in a smaller number of iterations and do not zigzag as much.\n",
    "\n",
    "2. When the first-order methods get close to the optimum, the rate of convergence typically slows down. When second-order methods get close to the optimum, the rate of convergence often speeds up, with the number of significant digits of accuracy doubling (!) with each iteration.  This fast convergence is called **quadratic convergence**.\n",
    "\n",
    "3. First-order methods work differently depending on how the data are scaled.  If some of the data are in millimeters and other data are in kilometers, then the algorithm will behave differently, even in exact arithmetic, compared to scaling all of the data in kilometers. With second-order methods, the scaling of the variables does not affect the results in exact arithmetic but may affect rounding error.\n",
    "\n",
    "4. Calculating the hessian matrix is much more expensive than calculating a gradient matrix.  The hessian matrix may also be impractically large to calculate. Gradients are much easier to calculate.\n",
    "\n",
    "5. If a gradient method is used with very small steps, the algorithm tends to avoid **saddle points**, instead converging to a local maximum or minimum, which is likely desired. A second-order method is thrown off if the problem is not locally concave (for optimization) or convex (for minimization) and may get stuck at a saddle point.\n",
    "\n",
    "All of these considerations play out differently when comparing machine learning algorithms (such as estimating large language models) with typical finance problems.\n",
    "\n",
    "When estimating some machine learning problems, such as large language models, the number of parameters can be huge, in the millions.  This makes the theoretical hession have more than on trillion ($10^{12}$) entries, which would require about 4 TB simply to store. The hessian is also computationally impractical to compute. As a result, these big machine learning optimization algorithms tend to use variations of first-order methods.  Of course, the results may not be very accurate, but this is not a major problem for some models because highly accurate solutions might involve overfitting the model anyway.\n",
    "\n",
    "When estimating finance models, the number of parameters for a large problem is typically in the thousands, not millions.  This makes it potentially practical to calculate the hessian. Furthermore, finance problems may need highly accurate solutions for regulatory, compliance, or practical trading implementation.\n",
    "\n",
    "Large language models are typically unconstrained optimization problems.  Finance problems often have equality and inequality constraints.  Constrained problems require different algorithms to handle the constraints.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Numerical derivatives\n",
    "\n",
    "In many cases, the objective function is sufficiently complicated that differentiating the objective by hand, then programming up the derivative in say Python, is time-consuming and error-prone. Therefore, many algorithms use **numerical derivatives** or **numerical gradients** instead of exact functional forms.\n",
    "\n",
    "For a function of one variable, $f(x)$, the numerical derivative at point $x$ may be calculated by picking a small value $\\Delta x$, then mimicking the definition of a derivative to calculate the approximation\n",
    "\n",
    "$$\n",
    "f'(x) \\approx \\frac{\\Delta f(x)}{\\Delta x} := \\frac{f(x + \\Delta x) - f(x)}{\\Delta x}.\n",
    "$$\n",
    "\n",
    "Mathematically, this approximation converges to the derivative in the limit as $\\Delta x \\rightarrow 0$.\n",
    "\n",
    "Numerically, the use of numerical derivatives raises several issues:\n",
    "\n",
    "1. If $\\Delta x$ is not small enough, the approximation may not be accurate enough. The approximation depends on whether $\\Delta x$ is chosen to be a small positive number or small negative number.\n",
    "\n",
    "2. If $\\Delta x$ is made very small, significant numerical error results from the subtraction of two numbers of approximately equal size in the numerator.\n",
    "\n",
    "This tradeoff between theoretical accuracy and numerical error has been carefully studied. As a rule of thumb, a good choice for $\\Delta x$ is $\\Delta x \\approx x \\sm \\sqrt{\\epsilon_{\\text{machine}}}$, where $\\epsilon_\\text{machine}$ is the smallest floating point number which can be represented given the numerical dtype used. For `np.float64`, $\\epsilon_\\text{machine} \\approx 10^{-16}$, so $\\Delta x \\approx x \\sm 10^{-8}$ is a reasonable choice. Similarly, $\\Delta x \\approx x \\sm 10^{-4}$ is a reasonable choice for `np.float32`.\n",
    "\n",
    "To improve accuracy, there are a few special tricks:\n",
    "\n",
    "1. Use a **central difference** (**two-sided approximation**) as follows:\n",
    "\n",
    "$$\n",
    "f'(x) \\approx  \\frac{f(x + \\Delta x) - f(x - \\Delta x)}{2 \\sm \\Delta x}.\n",
    "$$\n",
    "\n",
    "This makes the approximation more accurate in exact arithmetic. This can be proved by comparing the two-sided approximation with a theoretically-more-accurate two-sided approximation based on the second-order Taylor series approximation rather than the first-order approximation implicitly used. This comparison shows that the second-derivative term cancels out, which makes the above first-order approximation exactly mathematically equal to the theoretically-more-accurate second-order approximation. When using the central difference formula the best value of $\\Delta x$ may be based on the cube root of the **machine epsilon** $x \\sm \\epsilon_{\\text{machine}}^{1/3}$ rather than the square root.\n",
    "\n",
    " 2. Given a choice for $\\Delta x$, it may be more mathematically accurate to redefine $\\hat{\\Delta x}$ as $\\hat{\\Delta x} := (x + \\Delta x) - x$, which might be slightly different from $\\Delta x$ when represented as a floating point dtype like `np.int64`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2\n",
    "\n",
    "A. Consider the function $f(x) = \\log(x)$. Construct a small pandas dataframe in which the rows correspond to choices of $\\Delta x = x \\sm \\epsilon$ where $\\epsilon$ is in $[10^{-2}, 10^{-3}, \\ldots, 10^{-20}]$, and the columns are calculated as follows:\n",
    "\n",
    "1. The values of $\\epsilon$. \n",
    "\n",
    "2. The vvalue of $\\Delta x4.\n",
    "\n",
    "3. The error in the one sided approximation with $\\Delta x > 0$ and $\\Delta x$ in the denominator. \n",
    "\n",
    "4. The error in the one-sided approximation with $\\Delta x < 0$ (obtained by multiplying $\\Delta x$ by $-1$) with $\\Delta x$ in the denominator.\n",
    "\n",
    "5. The error in the two-sided approximation with $\\Delta x$ in the denominator.\n",
    "\n",
    "6. The error in the one sided approximation with $\\Delta x > 0$ and $(x + \\Delta x) - x$ in the denominator. \n",
    "\n",
    "7. The error in the one-sided approximation with $\\Delta x < 0$ (obtained by multiplying $\\Delta x$ by $-1$) with $(x + \\Delta x) - x$ in the denominator.\n",
    "\n",
    "8. The error in the two-sided approximation with $(x + \\Delta x) - (x - \\Delta x)$ in the denominator.\n",
    "\n",
    "Use the value $x = 333.0$.\n",
    "\n",
    "B. Try other values for $x$. Which column of approximations is most accurate? Do your results favor using $x \\sm \\sqrt{\\epsilon_{\\text{machine}}}$ (or the cube root for two-sided approximations)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Automatic derivatives\n",
    "\n",
    "Over the past two or three decades, calculating derivatives has been a major obstacle for optimization algorithms.  On the one hand, writing down an exact formula for a gradient or hession is time-consuming, error-prone, and impossibly complicated for complicated objective functions with a large number of parameters. On the other hand, numerical derivatives are both inaccurate and computationally very slow. For example, if there are 1000 parameters, a two-sided numerical gradient requires 2000 function evaluations, and a numerical hession may require about 1,000,000 function evaluations.\n",
    "\n",
    "In recent decades, a third alternative has become available: **automatic derivatives**. Automatic differentiation is an algorithm in which the computer applies rules for differentiation (chain rule, rules for sums and products, rules for log and exp, etc.) internally to obtain gradient function which would be accurate in exact arithmetic.  The result is not a formula which the user can look at.  Instead, the result is a compiled function which can be furhter optimized (by a compiler) to calculate derivatives both quickly and accurately.\n",
    "\n",
    "Automatic differentiation is currently an active area of research in software development in computer science. While it is obviously very important for fast estimation of large language models, it is also very useful for finance applications.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graphics cards\n",
    "\n",
    "The computations needed in continuous optimization are well-suited to graphics cards because many of the computations involve repetitive calculations on large matrices.\n",
    "\n",
    "Since graphics cards can potentially speed up estimation of models by orders of magnitude, developing software for implementing automatic differentiation on graphics cards has become imortant.\n",
    "\n",
    "For very large problems, many CPUs or many GPUs may be required to obtain solutions fast enough."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Just-in-time compiling (JIT)\n",
    "\n",
    "The compilation of automatic derivatives fits well with the trend of **just-in-time** compiling (**JIT**) as an alternative to either slow interpretation of code or **ahead-of-time** compiling (**AOT**).\n",
    "\n",
    "In both finance and machine learning, JIT compiling is needed because algorithms may be chosen or tweaked programatically as instructions execute.\n",
    "\n",
    "Combining together efficient matrix algebra implementations, automatic derivatives, implementations on graphics cards, and JIT-compiling requires significant programming effort. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sparse matrices and vectors\n",
    "\n",
    "A **sparse** vector, matrix, or array is defined as an array in which a large percentage of the elements are known to be exactly zero. If the percentage of nonzero elements is small enough, an array object which stores only the location and value of nonzero elements can save storage and also make computation faster.\n",
    "\n",
    "Sparse arrays can arise in finance problems. Sparse arrays can also arise in optimization problems. \n",
    "\n",
    "Developing software for making efficient calculations with sparse arrays is an active area of current research."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loaders\n",
    "\n",
    "Some maching learning models require large amounts of input data which are fed to the algorithm in batches. This creates a need for algorithms which efficiently load batches of data to an optimizer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functional programming\n",
    "\n",
    "GPUs execute code asynchronously. This makes it difficult for a compiler to optimize code when functions have **side effects**, defined as functions changing objects which are not explicit arguments to the function. **Functional programming**, which requires objects which a function changes to be explicit function arguments, makes optimizing code execution easier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Packages: Pytorch and Jax\n",
    "\n",
    "For optimization in Python, two packages which can be used are `pytorch` (originally sponsored by Facebook) and `jax` (sponsored by Google as an alternative to `tensorflow`).\n",
    "\n",
    "Both packages implement fast matrix algebra, computation on GPUs, automatic differentiation, JIT compiling, and efficient dataloaders. Pytorch is better established, more fully-featured, and very popular among researchers because it is relatively easy to use. Jax uses functional programming and may be faster than pytorch. It is newer and therefore has fewer features. For full functionality with GPUs, both Pytorch and Jax requires linux.\n",
    "\n",
    "Both Jax and Pytorch have built-in optimization algorithms. These algorithms are designed for estimating large-scale machine learning models which have many parameters and require huge amounts of data. As a result of having many parameters, the algorithms focus on first-order methods and feature many variations of the gradient method. \n",
    "\n",
    "The built-in Jax and Pytorch optimizers have some shortcomings when applied to finance problems. Solutions obtained from gradient methods may be inaccurate. Furthermore, the algorithms do not typically deal with constraints.\n",
    "\n",
    "Both Pytorch and Jax can handle sparse arrays, but the functionality is currently limited.\n",
    "\n",
    "Python's Scipy package has optimization algorithms using `scipy.optimize.minimize`. Many different algorithms are available.  For finance problems, these optimizers offer decent functionality. The advantages are the linear and nonlinear constraints can be handled. The disadvantages are that automatic differentiation and execution on GPUs are not available. The user can add jit-compiling using the Python package Numba. There is some support for sparse arrays.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestamp: 2024-1023-1154\n",
      "Finished: timestamp = '2024-1023-1154'\n",
      "Execution time = 0.5507102999836206 s\n"
     ]
    }
   ],
   "source": [
    "timestamp = datetime.datetime.now().strftime('%Y-%m%d-%H%M')\n",
    "print(\"Timestamp:\", timestamp)\n",
    "tfinish = timeit.default_timer()\n",
    "print(f\"Finished: {timestamp = }\\nExecution time = {tfinish - tstart} s\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "a202402f",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
